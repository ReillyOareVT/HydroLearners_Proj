{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8c3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Tue Apr 22 20:02:04 2025\n",
    "\n",
    "@author: malif\n",
    "\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Streamflow and Meteorological Data Processing and Visualization\n",
    "\n",
    "Steps:\n",
    "1. Load and filter streamflow data by date and completeness\n",
    "2. Identify basins with full streamflow and meteorological data\n",
    "3. Resample daily meteorology to weekly\n",
    "4. Convert streamflow to long format\n",
    "5. Merge streamflow with meteorology\n",
    "6. Analyze yearly data completeness\n",
    "7. Visualize time trends and heatmaps\n",
    "8. Generate % missing data matrix per variable (basin x year)\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed25f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47c17b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_folder = r\"D:\\\\VT_SPR2025\\\\CEE5984_Machine Learning in Water Resources\\\\GroupProject\\\\01_EStreams_Data\\\\EStreams\\\\meteorology\"\n",
    "streamflow_path = r\"D:\\\\VT_SPR2025\\\\CEE5984_Machine Learning in Water Resources\\\\GroupProject\\\\01_EStreams_Data\\\\EStreams\\\\streamflow_indices\\\\weekly\\\\weekly_streamflow_mean.csv\"\n",
    "start_date = \"1950-01-01\"\n",
    "end_date = \"2020-12-31\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a44899",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_streamflow = pd.read_csv(streamflow_path, parse_dates=True, index_col=0)\n",
    "streamflow_filtered = weekly_streamflow.loc[start_date:end_date]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c85c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_streamflow_basins = streamflow_filtered.columns[streamflow_filtered.notna().mean() == 1].tolist()\n",
    "print(f\"\\nBasins with full streamflow data ({start_date} to {end_date}): {len(full_streamflow_basins)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bcd426",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_meteorology_weekly(df):\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    rules = {\n",
    "        'p_mean': 'sum', 'pet_mean': 'sum',\n",
    "        't_mean': 'mean', 't_min': 'mean', 't_max': 'mean',\n",
    "        'sp_mean': 'mean', 'rh_mean': 'mean', 'ws_mean': 'mean', 'swr_mean': 'mean'\n",
    "    }\n",
    "    return df.resample('W-SUN').agg(rules)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25a724",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_basins = []\n",
    "\n",
    "for basin_id in full_streamflow_basins:\n",
    "    meteo_path = os.path.join(meteo_folder, f\"estreams_meteorology_{basin_id}.csv\")\n",
    "    if not os.path.exists(meteo_path):\n",
    "        continue\n",
    "    df_meteo = pd.read_csv(meteo_path, parse_dates=True, index_col=0)\n",
    "    df_weekly = resample_meteorology_weekly(df_meteo.loc[start_date:end_date])\n",
    "    # if df_weekly.loc[start_date:end_date].isna().any().any():\n",
    "    #     continue\n",
    "    meteo_basins.append(basin_id)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fff82ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamflow_long = streamflow_filtered[meteo_basins].reset_index().melt(\n",
    "    id_vars=['index'], var_name='basin_id', value_name='streamflow'\n",
    ").rename(columns={'index': 'date'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff1ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "meteo_long_all = []\n",
    "for basin_id in meteo_basins:\n",
    "    path = os.path.join(meteo_folder, f\"estreams_meteorology_{basin_id}.csv\")\n",
    "    df = pd.read_csv(path, parse_dates=True, index_col=0).loc[start_date:end_date]\n",
    "    df = resample_meteorology_weekly(df).loc[start_date:end_date]\n",
    "    df['basin_id'] = basin_id\n",
    "    meteo_long_all.append(df.reset_index())\n",
    "\n",
    "meteo_long = pd.concat(meteo_long_all, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5020ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "estreams_merged_01 = pd.merge(streamflow_long, meteo_long, on=['date', 'basin_id'], how='inner')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44dd351",
   "metadata": {},
   "outputs": [],
   "source": [
    "estreams_merged_01['year'] = pd.to_datetime(estreams_merged_01['date']).dt.year\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb4c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['p_mean', 't_mean', 't_min', 't_max', 'sp_mean', 'rh_mean', 'ws_mean', 'swr_mean', 'pet_mean']\n",
    "years = sorted(estreams_merged_01['year'].unique())\n",
    "\n",
    "yearly_available = {var: [] for var in variables}\n",
    "yearly_missing = {var: [] for var in variables}\n",
    "\n",
    "for year in years:\n",
    "    df_year = estreams_merged_01[estreams_merged_01['year'] == year]\n",
    "    for var in variables:\n",
    "        available_pct = df_year[var].notna().mean() * 100\n",
    "        yearly_available[var].append(available_pct)\n",
    "        yearly_missing[var].append(100 - available_pct)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8e0c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 8))\n",
    "for var in variables:\n",
    "    plt.plot(years, yearly_missing[var], label=var)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Missing Data (%)')\n",
    "plt.title('Yearly % Missing Data Across All Basins')\n",
    "plt.legend(title='Variable')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae33e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap_df = pd.DataFrame(yearly_available, index=years).T\n",
    "plt.figure(figsize=(24, 10))\n",
    "sns.heatmap(\n",
    "    heatmap_df, cmap=\"YlGnBu\", annot=True, fmt=\".0f\",\n",
    "    annot_kws={\"size\": 8}, linewidths=0.5, linecolor='gray',\n",
    "    vmin=0, vmax=100, cbar_kws={'label': 'Data Availability (%)'}\n",
    ")\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Meteorological Variable')\n",
    "plt.title('Data Availability Over Time (Heatmap)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997471b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "basins = estreams_merged_01['basin_id'].unique()\n",
    "missing_matrix = {}\n",
    "\n",
    "for var in variables:\n",
    "    df_missing = pd.DataFrame(index=basins, columns=years)\n",
    "    for year in years:\n",
    "        year_data = estreams_merged_01[estreams_merged_01['year'] == year]\n",
    "        grouped = year_data.groupby('basin_id')[var].apply(lambda x: x.isna().mean() * 100)\n",
    "        df_missing[year] = grouped\n",
    "    missing_matrix[var] = df_missing\n",
    "    # df_missing.to_csv(f\"missing_percentage_{var}.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8192e38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "# Path to static attribute files\n",
    "attributes_folder = r\"D:\\VT_SPR2025\\CEE5984_Machine Learning in Water Resources\\GroupProject\\01_EStreams_Data\\EStreams\\attributes\\static_attributes\"\n",
    "\n",
    "# List of static files to merge\n",
    "static_files = [\n",
    "    'estreams_topography_attributes.csv',\n",
    "    'estreams_soil_attributes.csv',\n",
    "    'estreams_geology_attributes.csv',\n",
    "    'estreams_hydrology_attributes.csv',\n",
    "    'estreams_vegetation_attributes.csv',\n",
    "    'estreams_snowcover_attributes.csv',\n",
    "    'estreams_landcover_attributes.csv',\n",
    "    'estreams_geologycontinental_attributes.csv'\n",
    "]\n",
    "\n",
    "# Read and store each static dataframe\n",
    "static_dfs = []\n",
    "for file in static_files:\n",
    "    path = os.path.join(attributes_folder, file)\n",
    "    if os.path.exists(path):\n",
    "        df_static = pd.read_csv(path)\n",
    "        static_dfs.append(df_static)\n",
    "\n",
    "# Merge all static attributes on 'basin_id'\n",
    "combined_static_attributes = reduce(\n",
    "    lambda left, right: pd.merge(left, right, on='basin_id', how='outer'),\n",
    "    static_dfs\n",
    ")\n",
    "\n",
    "# Preview\n",
    "# print(\"\\nCombined static attributes:\")\n",
    "# print(combined_static_attributes.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1dedb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_attr_list = combined_static_attributes.columns.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9dd5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Desired attribute columns to retain\n",
    "selected_static_columns = [\n",
    "    'basin_id',             # join key\n",
    "    'ele_mt_mean',          # Mean elevation\n",
    "    'slp_dg_mean',          # Mean slope\n",
    "    # 'catchment_area',       # Area\n",
    "    'p_mean',               # Long-term average precipitation\n",
    "    'pet_mean',             # Long-term average PET\n",
    "    'aridity',              # Aridity index\n",
    "    'p_seasonality',        # Seasonality\n",
    "    'frac_snow',            # Snowfall fraction\n",
    "    'soil_tawc',            # Soil water capacity\n",
    "    'soil_bd',              # Bulk density\n",
    "    'soil_fra_sand',        # Soil sand fraction\n",
    "    'soil_fra_clay',        # Soil clay fraction\n",
    "    'lit_dom',              # Lithology class\n",
    "    'lulc_dom',             # Land use class\n",
    "    'ndvi_mean',            # Mean NDVI\n",
    "    'lai_mean'              # Mean LAI\n",
    "]\n",
    "\n",
    "# Subset the combined static attribute table\n",
    "static_selected = combined_static_attributes[selected_static_columns].drop_duplicates(subset='basin_id')\n",
    "\n",
    "# Merge with estreams_merged_01\n",
    "estreams_merged_02= pd.merge(\n",
    "    estreams_merged_01,\n",
    "    static_selected,\n",
    "    on='basin_id',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Preview\n",
    "# print(\"\\nMerged dataset with static attributes:\")\n",
    "# print(estreams_merged_02.head())\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
